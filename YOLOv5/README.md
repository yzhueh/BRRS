
  YOLOv5s used a feature fusion network (FFN) as its neck, which combined feature maps of different sizes and reduced their dimensions using convolutional layers. The prediction head was responsible for predicting the class and location of objects in the input image. YOLOv5 used a set of convolutional layers to generate a set of bounding boxes and class probabilities for each grid cell in the feature maps. These predictions were then filtered using non-maximum suppression to remove overlapping detections and produce the final output (Figure 5B). Next, it used a focus structure to reduce computation and improve accuracy by processing smaller image patches. 
  After training the segmentation model, we used the YOLOv5s box loss function to calculate the difference between the predicted bounding boxes and the ground truth bounding boxes (Figure 5C). The ground truth bounding boxes were manually circled out using ImageJ. The box function gradually converged to 0 both in the training dataset and validation dataset, indicating that our model achieved an acceptable performance in band segmentation. The same phenomenon was observed in the object loss function, which was used to compute the loss associated with the presence or absence of an object in a bounding box. Confusion analysis of 1000 trials displayed a 98% true positive rate for test band recognition and a 100% true negative rate for background classification, confirming that our model was able to precisely recognize test bands within LFA strips (Figure 5D).
